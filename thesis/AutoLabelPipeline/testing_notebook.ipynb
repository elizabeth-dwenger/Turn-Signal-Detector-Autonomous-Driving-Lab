{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn Signal Detection - Testing & Visualization Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Video\n",
    "import json\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "from utils.config import load_config\n",
    "from data import (\n",
    "    load_dataset_from_config,\n",
    "    create_image_loader,\n",
    "    SequencePreprocessor\n",
    ")\n",
    "from models import load_model\n",
    "from postprocess import create_postprocessor\n",
    "\n",
    "# Setup plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Suppress some warnings\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "CONFIG_FILE = \"configs/cosmos_reason1_video.yaml\"\n",
    "\n",
    "# Test sequences - add specific sequence IDs you want to test\n",
    "TEST_SEQUENCES = [\n",
    "    # Add your test sequence IDs here, or leave empty to use first N sequences\n",
    "    # Example:\n",
    "    # \"2024-03-25-15-40-16_mapping_tartu/camera_fl_2_track_5\",\n",
    "    # \"2024-03-25-15-40-16_mapping_tartu/camera_fl_2_track_10\",\n",
    "]\n",
    "\n",
    "NUM_SEQUENCES = 5  # Number of sequences to load if TEST_SEQUENCES is empty\n",
    "\n",
    "# Load config\n",
    "config = load_config(CONFIG_FILE)\n",
    "\n",
    "# Override to use test sequences\n",
    "if TEST_SEQUENCES:\n",
    "    config.data.sequence_filter = TEST_SEQUENCES\n",
    "    config.data.max_sequences = None\n",
    "else:\n",
    "    config.data.max_sequences = NUM_SEQUENCES\n",
    "\n",
    "print(f\"Configuration: {CONFIG_FILE}\")\n",
    "print(f\"Model: {config.model.type.value} ({config.model.inference_mode.value} mode)\")\n",
    "print(f\"Prompt: {config.model.prompt_template_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset_from_config(config.data)\n",
    "\n",
    "print(f\"Loaded {dataset.num_sequences} sequences\")\n",
    "print(f\"Total frames: {dataset.total_frames}\")\n",
    "print(f\"\\nSequences:\")\n",
    "for i, seq in enumerate(dataset.sequences):\n",
    "    gt_label = seq.ground_truth_label if seq.has_ground_truth else \"N/A\"\n",
    "    print(f\"  {i+1}. {seq.sequence_id[:60]:60s} | {seq.num_frames:3d} frames | GT: {gt_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup image loader (lazy)\n",
    "image_loader = create_image_loader(config.data, lazy=True)\n",
    "preprocessor = SequencePreprocessor(config.preprocessing)\n",
    "\n",
    "print(\"✓ Image loader and preprocessor ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model & Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View current prompt\n",
    "with open(config.model.prompt_template_path, 'r') as f:\n",
    "    current_prompt = f.read()\n",
    "\n",
    "print(\"Current Prompt:\")\n",
    "print(\"=\"*80)\n",
    "print(current_prompt)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test a new prompt without modifying files\n",
    "# Uncomment and edit to experiment with prompt variations\n",
    "\n",
    "# NEW_PROMPT = \"\"\"\n",
    "# You are analyzing a sequence of images showing a vehicle from behind.\n",
    "# Your task is to determine if the vehicle's turn signals are active.\n",
    "# \n",
    "# Analyze the entire sequence and respond with:\n",
    "# {\n",
    "#   \"label\": \"left\" | \"right\" | \"both\" | \"none\",\n",
    "#   \"confidence\": 0.0 to 1.0,\n",
    "#   \"reasoning\": \"Brief explanation\",\n",
    "#   \"start_frame\": frame number where signal starts (or null),\n",
    "#   \"end_frame\": frame number where signal ends (or null)\n",
    "# }\n",
    "# \"\"\"\n",
    "# \n",
    "# # This will override the prompt for testing\n",
    "# config.model.prompt_template_path = \"/tmp/test_prompt.txt\"\n",
    "# Path(\"/tmp/test_prompt.txt\").write_text(NEW_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model... (this may take a minute)\")\n",
    "model = load_model(config.model, warmup=True)\n",
    "print(\"✓ Model loaded and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference on Test Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all test sequences\n",
    "results = []\n",
    "\n",
    "for i, sequence in enumerate(dataset.sequences):\n",
    "    print(f\"\\nProcessing {i+1}/{dataset.num_sequences}: {sequence.sequence_id}\")\n",
    "    \n",
    "    # Load images\n",
    "    image_loader(sequence, load_full_frame=False)\n",
    "    loaded = sum(1 for f in sequence.frames if f.crop_image is not None)\n",
    "    print(f\"  Loaded {loaded}/{sequence.num_frames} images\")\n",
    "    \n",
    "    if loaded == 0:\n",
    "        print(\"  ⚠️  No images loaded, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Preprocess\n",
    "    if config.model.inference_mode.value == 'video':\n",
    "        video = preprocessor.preprocess_for_video(sequence)\n",
    "        print(f\"  Video shape: {video.shape}\")\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict_video(video)\n",
    "        predictions = [prediction]\n",
    "    else:\n",
    "        samples = preprocessor.preprocess_for_single_images(sequence)\n",
    "        images = [s[0] for s in samples]\n",
    "        print(f\"  Processing {len(images)} frames\")\n",
    "        \n",
    "        predictions = model.predict_batch(images)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'sequence': sequence,\n",
    "        'sequence_id': sequence.sequence_id,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': sequence.ground_truth_label if sequence.has_ground_truth else None\n",
    "    })\n",
    "    \n",
    "    # Show prediction\n",
    "    if config.model.inference_mode.value == 'video':\n",
    "        pred = predictions[0]\n",
    "        print(f\"  ✓ Prediction: {pred['label']} (conf: {pred['confidence']:.2f})\")\n",
    "        print(f\"    Latency: {pred['latency_ms']:.1f} ms\")\n",
    "        if pred.get('reasoning'):\n",
    "            print(f\"    Reasoning: {pred['reasoning'][:100]}...\")\n",
    "    \n",
    "    # Clear memory\n",
    "    if hasattr(image_loader, 'clear_sequence'):\n",
    "        image_loader.clear_sequence(sequence)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(results)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_data = []\n",
    "for result in results:\n",
    "    if config.model.inference_mode.value == 'video':\n",
    "        pred = result['predictions'][0]\n",
    "        summary_data.append({\n",
    "            'Sequence': result['sequence_id'][:50],\n",
    "            'Frames': result['sequence'].num_frames,\n",
    "            'Predicted': pred['label'],\n",
    "            'Confidence': f\"{pred['confidence']:.2f}\",\n",
    "            'Ground Truth': result['ground_truth'] or 'N/A',\n",
    "            'Correct': '✓' if pred['label'] == result['ground_truth'] else '✗' if result['ground_truth'] else '-'\n",
    "        })\n",
    "    else:\n",
    "        # For single-image mode, show most common prediction\n",
    "        labels = [p['label'] for p in result['predictions']]\n",
    "        most_common = Counter(labels).most_common(1)[0][0]\n",
    "        avg_conf = np.mean([p['confidence'] for p in result['predictions']])\n",
    "        summary_data.append({\n",
    "            'Sequence': result['sequence_id'][:50],\n",
    "            'Frames': result['sequence'].num_frames,\n",
    "            'Predicted': most_common,\n",
    "            'Confidence': f\"{avg_conf:.2f}\",\n",
    "            'Ground Truth': result['ground_truth'] or 'N/A',\n",
    "            'Correct': '✓' if most_common == result['ground_truth'] else '✗' if result['ground_truth'] else '-'\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize a sequence\n",
    "def visualize_sequence(result_idx, show_frames=True, max_frames=20):\n",
    "    \"\"\"\n",
    "    Visualize predictions for a sequence.\n",
    "    \n",
    "    Args:\n",
    "        result_idx: Index in results list\n",
    "        show_frames: Whether to show sample frames\n",
    "        max_frames: Maximum number of frames to display\n",
    "    \"\"\"\n",
    "    result = results[result_idx]\n",
    "    sequence = result['sequence']\n",
    "    predictions = result['predictions']\n",
    "    \n",
    "    print(f\"Sequence: {result['sequence_id']}\")\n",
    "    print(f\"Frames: {sequence.num_frames}\")\n",
    "    print(f\"Ground Truth: {result['ground_truth'] or 'N/A'}\")\n",
    "    print()\n",
    "    \n",
    "    # Reload images for visualization\n",
    "    image_loader(sequence, load_full_frame=False)\n",
    "    \n",
    "    if config.model.inference_mode.value == 'video':\n",
    "        # Video mode - single prediction\n",
    "        pred = predictions[0]\n",
    "        print(f\"Prediction: {pred['label']} (confidence: {pred['confidence']:.2f})\")\n",
    "        print(f\"Latency: {pred['latency_ms']:.1f} ms\")\n",
    "        \n",
    "        if pred.get('reasoning'):\n",
    "            print(f\"\\nReasoning:\\n{pred['reasoning']}\")\n",
    "        \n",
    "        if pred.get('raw_output'):\n",
    "            print(f\"\\nRaw Output:\\n{pred['raw_output'][:500]}...\")\n",
    "        \n",
    "        # Show sample frames\n",
    "        if show_frames:\n",
    "            frames_to_show = min(max_frames, len(sequence.frames))\n",
    "            indices = np.linspace(0, len(sequence.frames)-1, frames_to_show, dtype=int)\n",
    "            \n",
    "            cols = 5\n",
    "            rows = (frames_to_show + cols - 1) // cols\n",
    "            \n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(16, rows*3))\n",
    "            axes = axes.flatten() if frames_to_show > 1 else [axes]\n",
    "            \n",
    "            for i, idx in enumerate(indices):\n",
    "                frame = sequence.frames[idx]\n",
    "                if frame.crop_image is not None:\n",
    "                    axes[i].imshow(frame.crop_image)\n",
    "                    axes[i].set_title(f\"Frame {frame.frame_id}\\n{pred['label']}\")\n",
    "                    axes[i].axis('off')\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for i in range(frames_to_show, len(axes)):\n",
    "                axes[i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        # Single-image mode - timeline\n",
    "        labels = [p['label'] for p in predictions]\n",
    "        confidences = [p['confidence'] for p in predictions]\n",
    "        frame_ids = [f.frame_id for f in sequence.frames[:len(predictions)]]\n",
    "        \n",
    "        # Plot timeline\n",
    "        fig = plt.figure(figsize=(16, 6))\n",
    "        gs = GridSpec(2, 1, height_ratios=[2, 1], hspace=0.3)\n",
    "        \n",
    "        # Labels\n",
    "        ax1 = plt.subplot(gs[0])\n",
    "        label_map = {'none': 0, 'left': 1, 'right': 2, 'both': 3}\n",
    "        label_nums = [label_map.get(l, 0) for l in labels]\n",
    "        \n",
    "        ax1.plot(frame_ids, label_nums, 'o-', linewidth=2, markersize=6)\n",
    "        ax1.set_yticks([0, 1, 2, 3])\n",
    "        ax1.set_yticklabels(['None', 'Left', 'Right', 'Both'])\n",
    "        ax1.set_xlabel('Frame ID')\n",
    "        ax1.set_ylabel('Prediction')\n",
    "        ax1.set_title(f'Predictions over Time - {result[\"sequence_id\"]}')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confidence\n",
    "        ax2 = plt.subplot(gs[1])\n",
    "        ax2.fill_between(frame_ids, confidences, alpha=0.5)\n",
    "        ax2.plot(frame_ids, confidences, 'b-', linewidth=2)\n",
    "        ax2.set_xlabel('Frame ID')\n",
    "        ax2.set_ylabel('Confidence')\n",
    "        ax2.set_ylim([0, 1])\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Statistics\n",
    "        label_counts = Counter(labels)\n",
    "        print(f\"\\nLabel Distribution:\")\n",
    "        for label, count in label_counts.most_common():\n",
    "            pct = count / len(labels) * 100\n",
    "            print(f\"  {label:8s}: {count:3d} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nAverage Confidence: {np.mean(confidences):.2f}\")\n",
    "        print(f\"Min Confidence: {np.min(confidences):.2f}\")\n",
    "        print(f\"Max Confidence: {np.max(confidences):.2f}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    if hasattr(image_loader, 'clear_sequence'):\n",
    "        image_loader.clear_sequence(sequence)\n",
    "\n",
    "print(\"✓ Visualization function ready\")\n",
    "print(\"\\nUsage: visualize_sequence(0)  # 0 = first sequence in results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first sequence\n",
    "if results:\n",
    "    visualize_sequence(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all sequences (use with caution for many sequences)\n",
    "# for i in range(len(results)):\n",
    "#     print(f\"\\n{'='*80}\\n\")\n",
    "#     visualize_sequence(i, show_frames=False)  # Set show_frames=True to see images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine raw model outputs\n",
    "def show_raw_output(result_idx):\n",
    "    \"\"\"Show the raw model output for debugging\"\"\"\n",
    "    result = results[result_idx]\n",
    "    \n",
    "    print(f\"Sequence: {result['sequence_id']}\")\n",
    "    print(f\"Ground Truth: {result['ground_truth'] or 'N/A'}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    if config.model.inference_mode.value == 'video':\n",
    "        pred = result['predictions'][0]\n",
    "        print(\"RAW MODEL OUTPUT:\")\n",
    "        print(\"=\"*80)\n",
    "        print(pred.get('raw_output', 'No raw output available'))\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\\nPARSED:\")\n",
    "        print(f\"  Label: {pred['label']}\")\n",
    "        print(f\"  Confidence: {pred['confidence']:.2f}\")\n",
    "        print(f\"  Reasoning: {pred.get('reasoning', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"Showing first 3 predictions:\")\n",
    "        for i, pred in enumerate(result['predictions'][:3]):\n",
    "            print(f\"\\nFrame {i+1}:\")\n",
    "            print(pred.get('raw_output', 'No raw output available'))\n",
    "            print(f\"  -> Parsed: {pred['label']} ({pred['confidence']:.2f})\")\n",
    "\n",
    "# Example usage:\n",
    "# show_raw_output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance metrics\n",
    "metrics = model.get_metrics()\n",
    "\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(\"=\"*80)\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        if 'rate' in key or 'pct' in key:\n",
    "            print(f\"  {key:30s}: {value:.2%}\")\n",
    "        else:\n",
    "            print(f\"  {key:30s}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key:30s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (if ground truth available)\n",
    "results_with_gt = [r for r in results if r['ground_truth'] is not None]\n",
    "\n",
    "if results_with_gt:\n",
    "    if config.model.inference_mode.value == 'video':\n",
    "        correct = sum(1 for r in results_with_gt \n",
    "                     if r['predictions'][0]['label'] == r['ground_truth'])\n",
    "        accuracy = correct / len(results_with_gt)\n",
    "        print(f\"\\nAccuracy: {correct}/{len(results_with_gt)} = {accuracy:.1%}\")\n",
    "    else:\n",
    "        # For single-image, use majority vote\n",
    "        correct = 0\n",
    "        for r in results_with_gt:\n",
    "            labels = [p['label'] for p in r['predictions']]\n",
    "            most_common = Counter(labels).most_common(1)[0][0]\n",
    "            if most_common == r['ground_truth']:\n",
    "                correct += 1\n",
    "        accuracy = correct / len(results_with_gt)\n",
    "        print(f\"\\nAccuracy (majority vote): {correct}/{len(results_with_gt)} = {accuracy:.1%}\")\n",
    "else:\n",
    "    print(\"\\nNo ground truth labels available for accuracy calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file for later analysis\n",
    "output_dir = Path(\"notebook_outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(output_dir / \"test_summary.csv\", index=False)\n",
    "\n",
    "# Save detailed results\n",
    "detailed_results = []\n",
    "for result in results:\n",
    "    detailed_results.append({\n",
    "        'sequence_id': result['sequence_id'],\n",
    "        'ground_truth': result['ground_truth'],\n",
    "        'predictions': result['predictions']\n",
    "    })\n",
    "\n",
    "with open(output_dir / \"test_results.json\", 'w') as f:\n",
    "    json.dump(detailed_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Results saved to {output_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
