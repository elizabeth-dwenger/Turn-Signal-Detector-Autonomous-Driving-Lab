{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn Signal Detection - Results Visualization Notebook\n",
    "\n",
    "This notebook visualizes results from test runs executed on the cluster via sbatch.\n",
    "\n",
    "## Workflow:\n",
    "1. **On Cluster**: Run `test_pipeline.py` or `compare_prompts.py` via sbatch\n",
    "2. **Results Saved**: Pipeline saves JSON results with predictions\n",
    "3. **In Notebook**: Load results and visualize predictions vs ground truth\n",
    "\n",
    "## Features:\n",
    "- Load pre-computed test results\n",
    "- Visualize predictions frame-by-frame\n",
    "- Compare multiple prompt results\n",
    "- Analyze failure cases\n",
    "- Generate comparison reports\n",
    "\n",
    "## Note on Turn Signals:\n",
    "Turn signals can blink on/off within a sequence. A sequence might have:\n",
    "- Frames 0-10: Signal OFF\n",
    "- Frames 11-30: Signal ON (blinking)\n",
    "- Frames 31-40: Signal OFF again\n",
    "\n",
    "For video mode, the model outputs a single label for the whole sequence.\n",
    "For single-image mode, each frame gets its own prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Setup plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# Directory where test_pipeline.py saved results\n",
    "RESULTS_DIR = \"results/qwen25_vl_video/test_runs\"\n",
    "\n",
    "# OR directory where compare_prompts.py saved results\n",
    "# RESULTS_DIR = \"prompt_results\"\n",
    "\n",
    "# Base directory for images\n",
    "IMAGE_BASE_DIR = \"/gpfs/space/projects/ml2024/\"\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Image base: {IMAGE_BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Browse Available Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path(RESULTS_DIR)\n",
    "\n",
    "# Find result files\n",
    "csv_files = list(results_path.rglob('*.csv'))\n",
    "json_files = list(results_path.rglob('*.json'))\n",
    "\n",
    "print(f\"Found in {results_path}:\")\n",
    "print(f\"  CSV files: {len(csv_files)}\")\n",
    "print(f\"  JSON files: {len(json_files)}\")\n",
    "\n",
    "# Show recent files\n",
    "if json_files:\n",
    "    print(f\"\\nRecent JSON files:\")\n",
    "    for f in sorted(json_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5]:\n",
    "        print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Predictions\n",
    "\n",
    "Load predictions from CSV or JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions_from_csv(csv_path):\n",
    "    \"\"\"Load predictions from CSV file\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Group by sequence (if sequence_id or similar column exists)\n",
    "    # Assuming CSV has: frame_id, label, confidence, etc.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_predictions_from_json(json_path):\n",
    "    \"\"\"Load predictions from JSON file\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load specific result file\n",
    "# Choose one:\n",
    "\n",
    "# Option 1: Load from CSV\n",
    "if csv_files:\n",
    "    result_file = csv_files[0]  # Or choose specific file\n",
    "    print(f\"Loading CSV: {result_file.name}\")\n",
    "    predictions_df = load_predictions_from_csv(result_file)\n",
    "    display(predictions_df.head(10))\n",
    "\n",
    "# Option 2: Load from JSON\n",
    "elif json_files:\n",
    "    # Filter out summary files, look for sequence-specific files\n",
    "    sequence_files = [f for f in json_files if 'dataset_summary' not in f.name and 'pipeline_report' not in f.name]\n",
    "    \n",
    "    if sequence_files:\n",
    "        result_file = sequence_files[0]\n",
    "        print(f\"Loading JSON: {result_file.name}\")\n",
    "        predictions_json = load_predictions_from_json(result_file)\n",
    "        \n",
    "        # Show structure\n",
    "        print(f\"\\nJSON keys: {list(predictions_json.keys())}\")\n",
    "        if 'predictions' in predictions_json:\n",
    "            print(f\"Number of predictions: {len(predictions_json['predictions'])}\")\n",
    "            print(f\"\\nFirst prediction:\")\n",
    "            print(json.dumps(predictions_json['predictions'][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load and return image as RGB array\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(image_path))\n",
    "        if img is not None:\n",
    "            return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def visualize_frame_prediction(image, prediction, ground_truth=None):\n",
    "    \"\"\"\n",
    "    Visualize a single frame with prediction overlay.\n",
    "    \n",
    "    Args:\n",
    "        image: numpy array (H, W, 3)\n",
    "        prediction: dict with 'label', 'confidence'\n",
    "        ground_truth: optional ground truth label\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Color coding\n",
    "    color_map = {\n",
    "        'none': 'gray',\n",
    "        'left': 'yellow',\n",
    "        'right': 'orange',\n",
    "        'both': 'red'\n",
    "    }\n",
    "    \n",
    "    label = prediction.get('label', 'none')\n",
    "    conf = prediction.get('confidence', 0.0)\n",
    "    color = color_map.get(label, 'white')\n",
    "    \n",
    "    # Add prediction text\n",
    "    text = f\"Pred: {label.upper()} ({conf:.2f})\"\n",
    "    ax.text(10, 30, text, fontsize=14, color=color, \n",
    "            bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "    \n",
    "    # Add ground truth if available\n",
    "    if ground_truth:\n",
    "        match = \"✓\" if label == ground_truth else \"✗\"\n",
    "        gt_color = 'lime' if label == ground_truth else 'red'\n",
    "        ax.text(10, 60, f\"GT: {ground_truth.upper()} {match}\", \n",
    "                fontsize=14, color=gt_color,\n",
    "                bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_sequence_timeline(predictions, ground_truth=None):\n",
    "    \"\"\"\n",
    "    Plot timeline of predictions for a sequence.\n",
    "    \n",
    "    Args:\n",
    "        predictions: list of prediction dicts\n",
    "        ground_truth: optional list of GT labels\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 6), \n",
    "                                    gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # Extract data\n",
    "    frames = list(range(len(predictions)))\n",
    "    labels = [p.get('label', 'none') for p in predictions]\n",
    "    confidences = [p.get('confidence', 0.0) for p in predictions]\n",
    "    \n",
    "    # Map labels to numbers\n",
    "    label_map = {'none': 0, 'left': 1, 'right': 2, 'both': 3}\n",
    "    label_nums = [label_map.get(l, 0) for l in labels]\n",
    "    \n",
    "    # Plot labels\n",
    "    ax1.plot(frames, label_nums, 'o-', linewidth=2, markersize=6, label='Prediction')\n",
    "    \n",
    "    # Plot ground truth if available\n",
    "    if ground_truth:\n",
    "        gt_nums = [label_map.get(gt, 0) for gt in ground_truth]\n",
    "        ax1.plot(frames, gt_nums, 's-', linewidth=1, markersize=4, \n",
    "                alpha=0.7, label='Ground Truth', color='green')\n",
    "    \n",
    "    ax1.set_ylabel('Signal State', fontsize=12)\n",
    "    ax1.set_yticks([0, 1, 2, 3])\n",
    "    ax1.set_yticklabels(['None', 'Left', 'Right', 'Both'])\n",
    "    ax1.set_title('Turn Signal Predictions Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot confidence\n",
    "    ax2.fill_between(frames, confidences, alpha=0.5, color='blue')\n",
    "    ax2.plot(frames, confidences, 'b-', linewidth=2)\n",
    "    ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Threshold (0.5)')\n",
    "    ax2.set_xlabel('Frame Index', fontsize=12)\n",
    "    ax2.set_ylabel('Confidence', fontsize=12)\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"✓ Visualization functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Visualize Specific Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURE SEQUENCE TO VISUALIZE ===\n",
    "\n",
    "# Sequence ID (find from your results)\n",
    "SEQUENCE_ID = \"2024-07-09-16-49-42_mapping_tartu_streets/camera_wide_right_170\" \n",
    "TRACK_ID = 170\n",
    "\n",
    "# Path to predictions JSON for this sequence\n",
    "pred_base = f\"qwen25_vl_20260204_113951/sequences/{SEQUENCE_ID.replace('/', '_')}__track_{TRACK_ID}.json\"\n",
    "PREDICTIONS_FILE = results_path / pred_base\n",
    "\n",
    "# Backward-compatible fallback (older naming)\n",
    "if not PREDICTIONS_FILE.exists():\n",
    "    legacy = results_path / f\"{SEQUENCE_ID.replace('/', '_')}_track_{TRACK_ID}.json\"\n",
    "    if legacy.exists():\n",
    "        PREDICTIONS_FILE = legacy\n",
    "\n",
    "print(f\"Sequence: {SEQUENCE_ID}\")\n",
    "print(f\"Track: {TRACK_ID}\")\n",
    "print(f\"Predictions file: {PREDICTIONS_FILE}\")\n",
    "print(f\"Exists: {PREDICTIONS_FILE.exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions for this sequence\n",
    "if PREDICTIONS_FILE.exists():\n",
    "    with open(PREDICTIONS_FILE, 'r') as f:\n",
    "        seq_data = json.load(f)\n",
    "    \n",
    "    predictions = seq_data.get('predictions', [])\n",
    "    print(f\"Loaded {len(predictions)} predictions\")\n",
    "    \n",
    "    # Show first prediction\n",
    "    if predictions:\n",
    "        print(f\"\\nFirst prediction:\")\n",
    "        print(json.dumps(predictions[0], indent=2))\n",
    "else:\n",
    "    print(f\"⚠️  File not found: {PREDICTIONS_FILE}\")\n",
    "    print(f\"\\nAvailable files:\")\n",
    "    for f in sorted(results_path.glob('*.json'))[:10]:\n",
    "        print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeline\n",
    "if predictions:\n",
    "    # Extract ground truth if available\n",
    "    ground_truth = None\n",
    "    if 'metadata' in seq_data and 'ground_truth' in seq_data['metadata']:\n",
    "        ground_truth = seq_data['metadata']['ground_truth']\n",
    "    \n",
    "    fig = plot_sequence_timeline(predictions, ground_truth)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    labels = [p['label'] for p in predictions]\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    for label, count in label_counts.most_common():\n",
    "        pct = count / len(predictions) * 100\n",
    "        print(f\"  {label:8s}: {count:3d} frames ({pct:5.1f}%)\")\n",
    "    \n",
    "    confidences = [p['confidence'] for p in predictions]\n",
    "    print(f\"\\nConfidence Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(confidences):.3f}\")\n",
    "    print(f\"  Std:  {np.std(confidences):.3f}\")\n",
    "    print(f\"  Min:  {np.min(confidences):.3f}\")\n",
    "    print(f\"  Max:  {np.max(confidences):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Frames with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV with image paths (from original tracking data)\n",
    "TRACKING_CSV = \"data/tracking_data.csv\"\n",
    "\n",
    "if Path(TRACKING_CSV).exists():\n",
    "    tracking_df = pd.read_csv(TRACKING_CSV)\n",
    "    \n",
    "    # Filter for this sequence\n",
    "    seq_df = tracking_df[\n",
    "        (tracking_df['sequence_id'] == SEQUENCE_ID) & \n",
    "        (tracking_df['track_id'] == TRACK_ID)\n",
    "    ].sort_values('frame_id')\n",
    "    \n",
    "    print(f\"Found {len(seq_df)} frames in tracking data\")\n",
    "    \n",
    "    # Show sample\n",
    "    display(seq_df[['frame_id', 'crop_path', 'true_label']].head())\n",
    "else:\n",
    "    print(f\"⚠️  Tracking CSV not found: {TRACKING_CSV}\")\n",
    "    seq_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frames at regular intervals\n",
    "if seq_df is not None and predictions:\n",
    "    num_samples = min(12, len(predictions))\n",
    "    sample_indices = np.linspace(0, len(predictions)-1, num_samples, dtype=int)\n",
    "\n",
    "    rows = (num_samples + 3) // 4\n",
    "    cols = min(4, num_samples)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, rows*4))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "\n",
    "    seq_df_by_id = seq_df.set_index('frame_id', drop=False)\n",
    "\n",
    "    for idx, frame_idx in enumerate(sample_indices):\n",
    "        pred = predictions[frame_idx]\n",
    "        pred_frame_id = pred.get('frame_id', frame_idx)\n",
    "\n",
    "        # Get image path from tracking data (match by frame_id if possible)\n",
    "        if pred_frame_id in seq_df_by_id.index:\n",
    "            frame_row = seq_df_by_id.loc[pred_frame_id]\n",
    "        else:\n",
    "            frame_row = seq_df.iloc[frame_idx]\n",
    "\n",
    "        crop_path = Path(IMAGE_BASE_DIR) / frame_row['crop_path']\n",
    "\n",
    "        # Load image\n",
    "        img = load_image(crop_path)\n",
    "\n",
    "        if img is not None:\n",
    "            axes[idx].imshow(img)\n",
    "\n",
    "            # Add prediction overlay\n",
    "            label = pred['label']\n",
    "            conf = pred['confidence']\n",
    "\n",
    "            color_map = {'none': 'gray', 'left': 'yellow', 'right': 'orange', 'both': 'red'}\n",
    "            color = color_map.get(label, 'white')\n",
    "\n",
    "            axes[idx].text(10, 30, f\"{label.upper()}\\n{conf:.2f}\", \n",
    "                          fontsize=10, color=color, weight='bold',\n",
    "                          bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "\n",
    "            # Add ground truth\n",
    "            if 'true_label' in frame_row and pd.notna(frame_row['true_label']):\n",
    "                gt = frame_row['true_label']\n",
    "                match = \"✓\" if label == gt else \"✗\"\n",
    "                gt_color = 'lime' if label == gt else 'red'\n",
    "                axes[idx].text(10, 60, f\"GT: {gt} {match}\", \n",
    "                              fontsize=9, color=gt_color,\n",
    "                              bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "\n",
    "            axes[idx].set_title(f\"Frame {pred_frame_id}\", fontsize=10)\n",
    "        else:\n",
    "            axes[idx].text(0.5, 0.5, 'Image not found', \n",
    "                          ha='center', va='center', transform=axes[idx].transAxes)\n",
    "\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(num_samples, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Multiple Prompt Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple prompt comparison results\n",
    "COMPARISON_DIR = \"prompt_results\"\n",
    "\n",
    "comparison_path = Path(COMPARISON_DIR)\n",
    "\n",
    "if comparison_path.exists():\n",
    "    result_files = list(comparison_path.glob('*.json'))\n",
    "    result_files = [f for f in result_files if 'comparison' not in f.name]\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for rf in result_files:\n",
    "        with open(rf, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Prompt': Path(data.get('prompt_file', '')).stem,\n",
    "            'Timestamp': data.get('timestamp', ''),\n",
    "            'Sequences': data.get('num_sequences', 0),\n",
    "            'Accuracy': data.get('accuracy', 0.0),\n",
    "            'Avg_Latency_ms': data.get('metrics', {}).get('avg_latency_ms', 0.0),\n",
    "            'Parse_Success': data.get('metrics', {}).get('parse_success_rate', 0.0)\n",
    "        })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "        \n",
    "        print(f\"Comparing {len(comparison_data)} prompt results:\\n\")\n",
    "        display(comparison_df)\n",
    "        \n",
    "        # Plot comparison\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        ax1.barh(comparison_df['Prompt'], comparison_df['Accuracy'])\n",
    "        ax1.set_xlabel('Accuracy')\n",
    "        ax1.set_title('Prompt Accuracy Comparison')\n",
    "        ax1.set_xlim([0, 1])\n",
    "        \n",
    "        # Latency comparison\n",
    "        ax2.barh(comparison_df['Prompt'], comparison_df['Avg_Latency_ms'])\n",
    "        ax2.set_xlabel('Average Latency (ms)')\n",
    "        ax2.set_title('Prompt Latency Comparison')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Comparison directory not found: {COMPARISON_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Failure Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sequences where prediction != ground truth\n",
    "# This requires loading results from compare_prompts.py format\n",
    "\n",
    "if comparison_path.exists():\n",
    "    # Load latest result\n",
    "    latest_result = sorted(result_files, key=lambda f: f.stat().st_mtime)[-1]\n",
    "    \n",
    "    with open(latest_result, 'r') as f:\n",
    "        result_data = json.load(f)\n",
    "    \n",
    "    if 'results' in result_data:\n",
    "        results_list = result_data['results']\n",
    "        \n",
    "        # Find failures\n",
    "        failures = []\n",
    "        \n",
    "        for r in results_list:\n",
    "            gt = r.get('ground_truth')\n",
    "            if gt:\n",
    "                # For video mode\n",
    "                if r['mode'] == 'video':\n",
    "                    pred_label = r['prediction']['label']\n",
    "                    if pred_label != gt:\n",
    "                        failures.append({\n",
    "                            'sequence_id': r['sequence_id'],\n",
    "                            'ground_truth': gt,\n",
    "                            'predicted': pred_label,\n",
    "                            'confidence': r['prediction']['confidence'],\n",
    "                            'reasoning': r['prediction'].get('reasoning', '')[:100]\n",
    "                        })\n",
    "        \n",
    "        if failures:\n",
    "            print(f\"\\nFound {len(failures)} failure cases:\\n\")\n",
    "            failures_df = pd.DataFrame(failures)\n",
    "            display(failures_df)\n",
    "        else:\n",
    "            print(\"\\n✓ No failures found! Perfect accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Analysis\n",
    "\n",
    "Save analysis results for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison table\n",
    "if 'comparison_df' in locals():\n",
    "    output_dir = Path('notebook_analysis')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    comparison_df.to_csv(output_dir / 'prompt_comparison.csv', index=False)\n",
    "    print(f\"✓ Saved prompt comparison to {output_dir / 'prompt_comparison.csv'}\")\n",
    "\n",
    "# Export failure cases\n",
    "if 'failures_df' in locals():\n",
    "    failures_df.to_csv(output_dir / 'failure_cases.csv', index=False)\n",
    "    print(f\"✓ Saved failure cases to {output_dir / 'failure_cases.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
