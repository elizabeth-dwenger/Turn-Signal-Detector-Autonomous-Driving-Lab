{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5253940-621d-44cf-a686-d64c5922c3e3",
   "metadata": {},
   "source": [
    "# OpenAI API Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1034b9-3363-4934-85a5-e1186fc46399",
   "metadata": {},
   "source": [
    "## Count and sample from labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13af2c6-1ac7-475f-889c-01601087b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eeac96-173e-45e7-8fb8-00046dd84b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAND_LABELED_PATH = \"../json_txt/hand_labeled_annotations_redo.json\"\n",
    "SAMPLED_OUT_PATH = \"../sample_100_for_openai_lights.json\"\n",
    "# SAMPLED_OUT_PATH = \"../sample_100_for_openai_lights_test.json\"\n",
    "\n",
    "\n",
    "with open(HAND_LABELED_PATH) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "combo_counts = Counter((d[\"turn_signal\"], d[\"tail_light\"]) for d in data)\n",
    "print(\"Class distribution:\")\n",
    "for combo, count in combo_counts.items():\n",
    "    print(f\"{combo}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f3391-0b01-4f4b-9892-8295cf4f8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_scheme = {\n",
    "    (\"none\", \"off\"): 30,\n",
    "    (\"left\", \"on\"): 15,\n",
    "    (\"left\", \"unclear\"): 6,\n",
    "    (\"left\", \"off\"): 20,\n",
    "    (\"right\", \"off\"): 20,\n",
    "    (\"right\", \"on\"): 15,\n",
    "    (\"right\", \"unclear\"): 10,\n",
    "    (\"none\", \"on\"): 7,\n",
    "    (\"none\", \"unclear\"): 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e9632-224c-4d4e-9d9e-89852cf45cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sampled = []\n",
    "# for combo, n in sampling_scheme.items():\n",
    "#     subset = [d for d in data if (d[\"turn_signal\"], d[\"tail_light\"]) == combo]\n",
    "#     if len(subset) < n:\n",
    "#         print(f\"Only found {len(subset)} samples for {combo}, taking all available.\")\n",
    "#     sampled.extend(random.sample(subset, min(len(subset), n)))\n",
    "\n",
    "# print(f\"\\nTotal sampled: {len(sampled)}\")\n",
    "# with open(SAMPLED_OUT_PATH, \"w\") as f:\n",
    "#     json.dump(sampled, f, indent=2)\n",
    "# print(f\"Saved sampled subset to {SAMPLED_OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc07d2",
   "metadata": {},
   "source": [
    "### Accuracy of turn_signal predictions per turn signal type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_turn_signal(results):\n",
    "    counts = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "    for r in results:\n",
    "        hand = r[\"hand_label\"][\"turn_signal\"]\n",
    "        pred = r[\"openai_label\"][\"turn_signal\"]\n",
    "        counts[hand][\"total\"] += 1\n",
    "        if hand == pred:\n",
    "            counts[hand][\"correct\"] += 1\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    for ts_type, vals in counts.items():\n",
    "        accuracy = vals[\"correct\"] / vals[\"total\"] if vals[\"total\"] > 0 else 0\n",
    "        accuracy_dict[ts_type] = accuracy\n",
    "\n",
    "    return accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb50b4e-f994-47c3-990e-7a87921b2f08",
   "metadata": {},
   "source": [
    "## Run the OpenAI API on the sampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5cccb-a508-4184-a96e-7bb112eef5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "SAMPLED_PATH = \"../json_txt/sample_100_for_openai_lights.json\"\n",
    "LOCAL_BASE = \"../sampled_images\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33fdee-10a5-43b9-9dbc-ff4d96e5246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        return base64.b64encode(img.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac0847-7bea-4180-8326-48b505e35c09",
   "metadata": {},
   "source": [
    "## gpt-4o-mini & prompt 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1e611-665f-4395-b687-0e90ef4b37c2",
   "metadata": {},
   "source": [
    "Prompt 1:\n",
    "\n",
    "```\n",
    "\"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "\"1. turn_signal — one of: left, right, unclear, none, both\\n\"\n",
    "\"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "\"Return only valid JSON in this format:\\n\"\n",
    "\"{\\n\"\n",
    "\"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "\"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "\"}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88525d9a-a520-40e1-90cf-ac45bb3c1d4f",
   "metadata": {},
   "source": [
    "Approx time: 7:44pm October 27, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5087d53-a1b2-4974-a16c-90cf3facdcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"../json_txt/gpt-4o-mini_prompt1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e5c05-21ee-4044-9acd-df59ae58aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(local_path):\n",
    "    img_b64 = encode_image(local_path)\n",
    "    prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "                    \"1. turn_signal — one of: left, right, unclear, none, both\\n\"\n",
    "                    \"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "                    \"Return only valid JSON in this format:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "                    \"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "                    \"}\"\n",
    "                ),\n",
    "            },\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[prompt],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {local_path}: {e}\")\n",
    "        return {\"turn_signal\": \"none\", \"tail_light\": \"not_visible\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4b775-fd96-44ff-b9bc-cd9af949b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAMPLED_PATH) as f:\n",
    "    sampled = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418c05f-bf42-4a21-ab1c-8a549543918b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for entry in tqdm(sampled, desc=\"Classifying images\"): \n",
    "    local_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    print(local_path)\n",
    "    result = classify_image(local_path)\n",
    "    results.append({\n",
    "        \"image\": entry[\"image\"],\n",
    "        \"hand_label\": {\n",
    "            \"turn_signal\": entry[\"turn_signal\"],\n",
    "            \"tail_light\": entry[\"tail_light\"]\n",
    "        },\n",
    "        \"openai_label\": result\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nDone! Saved {len(results)} results to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd909dbc-7ea2-4437-862d-3396f57e6db4",
   "metadata": {},
   "source": [
    "### Display results side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9237d43-8446-4383-90ab-d0c004137b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../json_txt/gpt-4o-mini_prompt1.json\"\n",
    "LOCAL_BASE = \"../sampled_images\"\n",
    "\n",
    "with open(RESULTS_PATH) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Display 12 images per page\n",
    "for i, entry in enumerate(results[:100]):\n",
    "    if i % 12 == 0:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(3, 4, (i % 12) + 1)\n",
    "    img_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    try:\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"Hand: Turn Signal={entry['hand_label']['turn_signal']} | Tail Light={entry['hand_label']['tail_light']}\\n\"\n",
    "            f\"OpenAI: Turn Signal={entry['openai_label']['turn_signal']} | Tail Light={entry['openai_label']['tail_light']}\",\n",
    "            fontsize=8\n",
    "        )\n",
    "    except Exception as e:\n",
    "        plt.title(f\"Missing image: {e}\")\n",
    "    if (i + 1) % 12 == 0 or i == len(results) - 1:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d55119-c01d-4f62-96f6-c76635997f4c",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda32c18-00b4-45b3-ad7e-0b88ced55e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../json_txt/gpt-4o-mini_prompt1.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "total = len(results)\n",
    "turn_correct = sum(r[\"hand_label\"][\"turn_signal\"] == r[\"openai_label\"][\"turn_signal\"] for r in results)\n",
    "tail_correct = sum(r[\"hand_label\"][\"tail_light\"] == r[\"openai_label\"][\"tail_light\"] for r in results)\n",
    "\n",
    "print(f\"Turn signal accuracy: {turn_correct / total:.2%}\")\n",
    "print(f\"Tail light accuracy: {tail_correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba495d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_turn_accuracy = accuracy_per_turn_signal(results)\n",
    "print(\"\\nTurn signal accuracy per type:\")\n",
    "for ts_type, acc in per_turn_accuracy.items():\n",
    "    print(f\"{ts_type}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00bde5-960a-44bc-ba63-1cd929507e66",
   "metadata": {},
   "source": [
    "## gpt-4o & prompt 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc91d7-dbdb-469e-a50e-5f8845f48253",
   "metadata": {},
   "source": [
    "Ran approx: 7:47pm October 27 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae87fb-0143-45ab-b3ed-12a9264c2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"../json_txt/gpt-4o_prompt1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1abb15-d14b-4863-b609-1166ec98eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(local_path):\n",
    "    img_b64 = encode_image(local_path)\n",
    "    prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "                    \"1. turn_signal — one of: left, right, unclear, none, both\\n\"\n",
    "                    \"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "                    \"Return only valid JSON in this format:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "                    \"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "                    \"}\"\n",
    "                ),\n",
    "            },\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[prompt],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {local_path}: {e}\")\n",
    "        return {\"turn_signal\": \"none\", \"tail_light\": \"not_visible\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a50bf8-0b4d-4a6c-b2a1-8c7d2b542215",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAMPLED_PATH) as f:\n",
    "    sampled = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d1122-8ce1-4d3c-bb66-c9f8f4db15e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for entry in tqdm(sampled, desc=\"Classifying images\"): \n",
    "    local_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    print(local_path)\n",
    "    result = classify_image(local_path)\n",
    "    results.append({\n",
    "        \"image\": entry[\"image\"],\n",
    "        \"hand_label\": {\n",
    "            \"turn_signal\": entry[\"turn_signal\"],\n",
    "            \"tail_light\": entry[\"tail_light\"]\n",
    "        },\n",
    "        \"openai_label\": result\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nDone! Saved {len(results)} results to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206045cc-5c76-4c3c-90c0-5cd7da6f45ae",
   "metadata": {},
   "source": [
    "### Display results side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb52f8d-b9e5-4d70-9643-f4d0c6e4daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../json_txt/gpt-4o_prompt1.json\"\n",
    "LOCAL_BASE = \"../sampled_images\"\n",
    "\n",
    "with open(RESULTS_PATH) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Display 12 images per page\n",
    "for i, entry in enumerate(results[:100]):\n",
    "    if i % 12 == 0:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(3, 4, (i % 12) + 1)\n",
    "    img_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    try:\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"Hand: Turn Signal={entry['hand_label']['turn_signal']} | Tail Light={entry['hand_label']['tail_light']}\\n\"\n",
    "            f\"OpenAI: Turn Signal={entry['openai_label']['turn_signal']} | Tail Light={entry['openai_label']['tail_light']}\",\n",
    "            fontsize=8\n",
    "        )\n",
    "    except Exception as e:\n",
    "        plt.title(f\"Missing image: {e}\")\n",
    "    if (i + 1) % 12 == 0 or i == len(results) - 1:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eec4a0-87c2-4b5e-ac7f-356fbab232a8",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69083dd5-4e12-482c-a8d4-f4c5efcab851",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../json_txt/gpt-4o_prompt1.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "total = len(results)\n",
    "turn_correct = sum(r[\"hand_label\"][\"turn_signal\"] == r[\"openai_label\"][\"turn_signal\"] for r in results)\n",
    "tail_correct = sum(r[\"hand_label\"][\"tail_light\"] == r[\"openai_label\"][\"tail_light\"] for r in results)\n",
    "\n",
    "print(f\"Turn signal accuracy: {turn_correct / total:.2%}\")\n",
    "print(f\"Tail light accuracy: {tail_correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e18ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_turn_accuracy = accuracy_per_turn_signal(results)\n",
    "print(\"\\nTurn signal accuracy per type:\")\n",
    "for ts_type, acc in per_turn_accuracy.items():\n",
    "    print(f\"{ts_type}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de003589-fd44-4310-b383-619534ad1f21",
   "metadata": {},
   "source": [
    "## gpt-4o-mini & prompt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda29219-d8e7-4cc6-94f6-3f5f373f3db8",
   "metadata": {},
   "source": [
    "Prompt 1:\n",
    "\n",
    "```\n",
    "\"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "\"1. turn_signal — one of: left, right, unclear, none \\n\"\n",
    "\"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "\"Return only valid JSON in this format:\\n\"\n",
    "\"{\\n\"\n",
    "\"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "\"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "\"}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a7fb9-dab6-438b-986f-573205ea852e",
   "metadata": {},
   "source": [
    "Removing the none and both options, as perhaps this \"confuses\" the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c381fb2-e0a5-49be-9693-d2089a5398da",
   "metadata": {},
   "source": [
    "Running at approx 7:54pm October 27, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7dd9c2-8276-4dd2-8677-899febf7716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"../json_txt/gpt-4o-mini_prompt2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d50f7-44e5-41b2-9d2b-c632a714a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(local_path):\n",
    "    img_b64 = encode_image(local_path)\n",
    "    prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "                    \"1. turn_signal — one of: left, right, unclear, none \\n\"\n",
    "                    \"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "                    \"Return only valid JSON in this format:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "                    \"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "                    \"}\"\n",
    "                ),\n",
    "            },\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[prompt],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {local_path}: {e}\")\n",
    "        return {\"turn_signal\": \"none\", \"tail_light\": \"not_visible\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcf08e-ed75-4ee2-b74f-f67e1e669197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAMPLED_PATH) as f:\n",
    "    sampled = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2f1e1-e9ea-4787-8b5e-5e05add7e7ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for entry in tqdm(sampled, desc=\"Classifying images\"): \n",
    "    local_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    print(local_path)\n",
    "    result = classify_image(local_path)\n",
    "    results.append({\n",
    "        \"image\": entry[\"image\"],\n",
    "        \"hand_label\": {\n",
    "            \"turn_signal\": entry[\"turn_signal\"],\n",
    "            \"tail_light\": entry[\"tail_light\"]\n",
    "        },\n",
    "        \"openai_label\": result\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nDone! Saved {len(results)} results to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c95453-fed0-4ab8-9520-e54e619392e8",
   "metadata": {},
   "source": [
    "### Display results side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0c6f2-4e16-47b7-8eff-cc06a8c61017",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../json_txt/gpt-4o-mini_prompt2.json\"\n",
    "LOCAL_BASE = \"../sampled_images\"\n",
    "\n",
    "with open(RESULTS_PATH) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Display 12 images per page\n",
    "for i, entry in enumerate(results[:100]):\n",
    "    if i % 12 == 0:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(3, 4, (i % 12) + 1)\n",
    "    img_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    try:\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"Hand: Turn Signal={entry['hand_label']['turn_signal']} | Tail Light={entry['hand_label']['tail_light']}\\n\"\n",
    "            f\"OpenAI: Turn Signal={entry['openai_label']['turn_signal']} | Tail Light={entry['openai_label']['tail_light']}\",\n",
    "            fontsize=8\n",
    "        )\n",
    "    except Exception as e:\n",
    "        plt.title(f\"Missing image: {e}\")\n",
    "    if (i + 1) % 12 == 0 or i == len(results) - 1:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9be77-08dd-4e4d-8112-6bc071beb6b7",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a7676-456f-49b9-970a-6d16ce4d020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../json_txt/gpt-4o-mini_prompt2.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "total = len(results)\n",
    "turn_correct = sum(r[\"hand_label\"][\"turn_signal\"] == r[\"openai_label\"][\"turn_signal\"] for r in results)\n",
    "tail_correct = sum(r[\"hand_label\"][\"tail_light\"] == r[\"openai_label\"][\"tail_light\"] for r in results)\n",
    "\n",
    "print(f\"Turn signal accuracy: {turn_correct / total:.2%}\")\n",
    "print(f\"Tail light accuracy: {tail_correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734b990-bd74-4759-b3e9-fa7e5f80f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_turn_accuracy = accuracy_per_turn_signal(results)\n",
    "print(\"\\nTurn signal accuracy per type:\")\n",
    "for ts_type, acc in per_turn_accuracy.items():\n",
    "    print(f\"{ts_type}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e47cd-0bc5-439f-b7c0-b7e7da1d7118",
   "metadata": {},
   "source": [
    "## gpt-4o & prompt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41e82b-b33f-4d42-b195-ee5046f4a1c1",
   "metadata": {},
   "source": [
    "Running at approx 7:56pm, October 27, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03326a-7b9f-4c9c-a95b-580f119e0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"../json_txt/gpt-4o_prompt2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608478a-e7c8-43d5-b0a3-5f2b7f325d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(local_path):\n",
    "    img_b64 = encode_image(local_path)\n",
    "    prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "                    \"1. turn_signal — one of: left, right, unclear, none \\n\"\n",
    "                    \"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "                    \"Return only valid JSON in this format:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "                    \"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "                    \"}\"\n",
    "                ),\n",
    "            },\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[prompt],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {local_path}: {e}\")\n",
    "        return {\"turn_signal\": \"none\", \"tail_light\": \"not_visible\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ffdbe-7abc-4439-af88-6420a2492bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAMPLED_PATH) as f:\n",
    "    sampled = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba8a61-00b4-4300-bed3-1ff78762bb93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for entry in tqdm(sampled, desc=\"Classifying images\"): \n",
    "    local_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    print(local_path)\n",
    "    result = classify_image(local_path)\n",
    "    results.append({\n",
    "        \"image\": entry[\"image\"],\n",
    "        \"hand_label\": {\n",
    "            \"turn_signal\": entry[\"turn_signal\"],\n",
    "            \"tail_light\": entry[\"tail_light\"]\n",
    "        },\n",
    "        \"openai_label\": result\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nDone! Saved {len(results)} results to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdeb5ee-b052-4735-8c26-e011034dcec0",
   "metadata": {},
   "source": [
    "### Display results side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268338ed-e1fc-49f7-b18e-cbc0b06c6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../json_txt/gpt-4o_prompt2.json\"\n",
    "LOCAL_BASE = \"../sampled_images\"\n",
    "\n",
    "with open(RESULTS_PATH) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Display 12 images per page\n",
    "for i, entry in enumerate(results[:100]):\n",
    "    if i % 12 == 0:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(3, 4, (i % 12) + 1)\n",
    "    img_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    try:\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"Hand: Turn Signal={entry['hand_label']['turn_signal']} | Tail Light={entry['hand_label']['tail_light']}\\n\"\n",
    "            f\"OpenAI: Turn Signal={entry['openai_label']['turn_signal']} | Tail Light={entry['openai_label']['tail_light']}\",\n",
    "            fontsize=8\n",
    "        )\n",
    "    except Exception as e:\n",
    "        plt.title(f\"Missing image: {e}\")\n",
    "    if (i + 1) % 12 == 0 or i == len(results) - 1:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321dc10-84af-4613-b72b-8ca300afc796",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30346740-0b1c-4fda-a29f-d0b375b80888",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../json_txt/gpt-4o_prompt2.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "total = len(results)\n",
    "turn_correct = sum(r[\"hand_label\"][\"turn_signal\"] == r[\"openai_label\"][\"turn_signal\"] for r in results)\n",
    "tail_correct = sum(r[\"hand_label\"][\"tail_light\"] == r[\"openai_label\"][\"tail_light\"] for r in results)\n",
    "\n",
    "print(f\"Turn signal accuracy: {turn_correct / total:.2%}\")\n",
    "print(f\"Tail light accuracy: {tail_correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c51c6-8c7f-44ed-a929-e7a7e68ef06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_turn_accuracy = accuracy_per_turn_signal(results)\n",
    "print(\"\\nTurn signal accuracy per type:\")\n",
    "for ts_type, acc in per_turn_accuracy.items():\n",
    "    print(f\"{ts_type}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb768679-d752-4136-9cec-76464b693a4d",
   "metadata": {},
   "source": [
    "## gpt-4o-mini & prompt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad24ce-0d1e-4a50-9b4a-53c34b60e09b",
   "metadata": {},
   "source": [
    "Prompt 1:\n",
    "\n",
    "```\n",
    "\"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "\"1. turn_signal — one of: left, right, unclear, none \\n\"\n",
    "\"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "\"A turn signal should be lit up and indicating which direction the car is planning to turn \\n\"\n",
    "\"A tail light is on if lights other than the turn signal are on. \\n\"\n",
    "\"Return only valid JSON in this format:\\n\"\n",
    "\"{\\n\"\n",
    "\"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "\"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "\"}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa098b42-1de0-4d8a-8e3c-66dcdd1b4e73",
   "metadata": {},
   "source": [
    "Removing the none and both options, as perhaps this \"confuses\" the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abb798-0cd7-4925-9021-85aa7459d352",
   "metadata": {},
   "source": [
    "Running at approx 8:03pm October 27, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbd0c2-84e3-471f-b750-7930f90adecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"../json_txt/gpt-4o-mini_prompt3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec8810-9c7e-484a-8703-365f5dc5eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(local_path):\n",
    "    img_b64 = encode_image(local_path)\n",
    "    prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "                    \"1. turn_signal — one of: left, right, unclear, none \\n\"\n",
    "                    \"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "                    \"A turn signal should be lit up and indicating which direction the car is planning to turn \\n\"\n",
    "                    \"A tail light is on if lights other than the turn signal are on. \\n\"\n",
    "                    \"Return only valid JSON in this format:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "                    \"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "                    \"}\"\n",
    "                ),\n",
    "            },\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[prompt],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {local_path}: {e}\")\n",
    "        return {\"turn_signal\": \"none\", \"tail_light\": \"not_visible\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8957c-6ad3-419f-a8e0-c5f6786c3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAMPLED_PATH) as f:\n",
    "    sampled = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b6d24-1398-444f-9272-b22a8baf4ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for entry in tqdm(sampled, desc=\"Classifying images\"): \n",
    "    local_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    print(local_path)\n",
    "    result = classify_image(local_path)\n",
    "    results.append({\n",
    "        \"image\": entry[\"image\"],\n",
    "        \"hand_label\": {\n",
    "            \"turn_signal\": entry[\"turn_signal\"],\n",
    "            \"tail_light\": entry[\"tail_light\"]\n",
    "        },\n",
    "        \"openai_label\": result\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nDone! Saved {len(results)} results to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa0f69-1636-4c9c-b288-797590d8e7e8",
   "metadata": {},
   "source": [
    "### Display results side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0728d-ee88-4537-bb80-3d8e017d5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../json_txt/gpt-4o-mini_prompt3.json\"\n",
    "LOCAL_BASE = \"../sampled_images\"\n",
    "\n",
    "with open(RESULTS_PATH) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Display 12 images per page\n",
    "for i, entry in enumerate(results[:100]):\n",
    "    if i % 12 == 0:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(3, 4, (i % 12) + 1)\n",
    "    img_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    try:\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"Hand: Turn Signal={entry['hand_label']['turn_signal']} | Tail Light={entry['hand_label']['tail_light']}\\n\"\n",
    "            f\"OpenAI: Turn Signal={entry['openai_label']['turn_signal']} | Tail Light={entry['openai_label']['tail_light']}\",\n",
    "            fontsize=8\n",
    "        )\n",
    "    except Exception as e:\n",
    "        plt.title(f\"Missing image: {e}\")\n",
    "    if (i + 1) % 12 == 0 or i == len(results) - 1:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55017a6-9f8f-4e85-ba0b-9f5227e91166",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936bd8f-9d9d-4a23-bf09-26e9093adc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../json_txt/gpt-4o-mini_prompt3.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "total = len(results)\n",
    "turn_correct = sum(r[\"hand_label\"][\"turn_signal\"] == r[\"openai_label\"][\"turn_signal\"] for r in results)\n",
    "tail_correct = sum(r[\"hand_label\"][\"tail_light\"] == r[\"openai_label\"][\"tail_light\"] for r in results)\n",
    "\n",
    "print(f\"Turn signal accuracy: {turn_correct / total:.2%}\")\n",
    "print(f\"Tail light accuracy: {tail_correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743adb09-30e7-4934-ac7a-88a043994434",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_turn_accuracy = accuracy_per_turn_signal(results)\n",
    "print(\"\\nTurn signal accuracy per type:\")\n",
    "for ts_type, acc in per_turn_accuracy.items():\n",
    "    print(f\"{ts_type}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916aaaaf-268d-4434-a078-d1b27bdf3422",
   "metadata": {},
   "source": [
    "## gpt-4o & prompt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3180d-c879-4558-944a-a834af6fb50e",
   "metadata": {},
   "source": [
    "Removing the none and both options, as perhaps this \"confuses\" the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733fcdde-a213-4c7b-8efe-0d6ba794874a",
   "metadata": {},
   "source": [
    "Running at approx 8:05, October 27, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a44617-ff88-4425-9d8e-9a8eba75f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"../json_txt/gpt-4o_prompt3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96153da-34f4-4312-b33a-40a5516e0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(local_path):\n",
    "    img_b64 = encode_image(local_path)\n",
    "    prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"You are an advanced image analysis model. Look at the car image and determine:\\n\"\n",
    "                    \"1. turn_signal — one of: left, right, unclear, none \\n\"\n",
    "                    \"2. tail_light — one of: on, off, unclear \\n\\n\"\n",
    "                    \"Return only valid JSON in this format:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"turn_signal\\\": \\\"left\\\",\\n\"\n",
    "                    \"  \\\"tail_light\\\": \\\"on\\\"\\n\"\n",
    "                    \"}\"\n",
    "                ),\n",
    "            },\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[prompt],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {local_path}: {e}\")\n",
    "        return {\"turn_signal\": \"none\", \"tail_light\": \"not_visible\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb7c40-a8a2-48ac-ac45-d7638604d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAMPLED_PATH) as f:\n",
    "    sampled = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f2619-6643-46fb-831b-5c8f5fd73b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for entry in tqdm(sampled, desc=\"Classifying images\"): \n",
    "    local_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    print(local_path)\n",
    "    result = classify_image(local_path)\n",
    "    results.append({\n",
    "        \"image\": entry[\"image\"],\n",
    "        \"hand_label\": {\n",
    "            \"turn_signal\": entry[\"turn_signal\"],\n",
    "            \"tail_light\": entry[\"tail_light\"]\n",
    "        },\n",
    "        \"openai_label\": result\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nDone! Saved {len(results)} results to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda210f4-a374-430c-8e20-6a1638524d8e",
   "metadata": {},
   "source": [
    "### Display results side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb67966-03c6-465c-b3b7-3785c9a8f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../json_txt/gpt-4o_prompt3.json\"\n",
    "LOCAL_BASE = \"../sampled_images\"\n",
    "\n",
    "with open(RESULTS_PATH) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Display 12 images per page\n",
    "for i, entry in enumerate(results[:100]):\n",
    "    if i % 12 == 0:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(3, 4, (i % 12) + 1)\n",
    "    img_path = os.path.join(LOCAL_BASE, entry[\"image\"].lstrip(\"/\"))\n",
    "    try:\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"Hand: Turn Signal={entry['hand_label']['turn_signal']} | Tail Light={entry['hand_label']['tail_light']}\\n\"\n",
    "            f\"OpenAI: Turn Signal={entry['openai_label']['turn_signal']} | Tail Light={entry['openai_label']['tail_light']}\",\n",
    "            fontsize=8\n",
    "        )\n",
    "    except Exception as e:\n",
    "        plt.title(f\"Missing image: {e}\")\n",
    "    if (i + 1) % 12 == 0 or i == len(results) - 1:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571eeea9-8d34-4032-94dd-d5fad8709833",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905350e-c611-4c8d-ae94-a6bf587534b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../json_txt/gpt-4o_prompt3.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "total = len(results)\n",
    "turn_correct = sum(r[\"hand_label\"][\"turn_signal\"] == r[\"openai_label\"][\"turn_signal\"] for r in results)\n",
    "tail_correct = sum(r[\"hand_label\"][\"tail_light\"] == r[\"openai_label\"][\"tail_light\"] for r in results)\n",
    "\n",
    "print(f\"Turn signal accuracy: {turn_correct / total:.2%}\")\n",
    "print(f\"Tail light accuracy: {tail_correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8f50b-2bc2-4e22-b298-2d335ae15692",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_turn_accuracy = accuracy_per_turn_signal(results)\n",
    "print(\"\\nTurn signal accuracy per type:\")\n",
    "for ts_type, acc in per_turn_accuracy.items():\n",
    "    print(f\"{ts_type}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507426c9-8d5f-4f33-8f0f-d3a627f9add5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
